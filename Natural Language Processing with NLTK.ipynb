{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n",
      "Pandas : 0.23.4\n",
      "Sklearn : 0.20.0\n",
      "NLTK :3.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import sklearn\n",
    "\n",
    "print(\"Python : {}\".format(sys.version))\n",
    "print(\"Pandas : {}\".format(pandas.__version__))\n",
    "print(\"Sklearn : {}\".format(sklearn.__version__))\n",
    "print(\"NLTK :{}\".format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # Opens up a nltk downloader with all the packages listed to be seleccted and donloaded easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of vocabulary with NLP\n",
    "\n",
    "### Corpus\n",
    "Body of the text,singular. Corpora is it's plural. ex: A collection of comic books\n",
    "\n",
    "### Lexicon\n",
    "Words and their meanings with respect to different organisations and types of people from various backgrounds\n",
    "\n",
    "### Token\n",
    "Single entitiy that has been split from the document to be used or has been split up based on the rules, example: each word is atoken when a sentence is tokenized. A sentence can be atoken when it's a part of a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text = \"Hey dude, what's up, how are you doing today! I think this project is amazing! It's a beautiful wworld.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey dude, what's up, how are you doing today!\", 'I think this project is amazing!', \"It's a beautiful wworld.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'dude', ',', 'what', \"'s\", 'up', ',', 'how', 'are', 'you', 'doing', 'today', '!', 'I', 'think', 'this', 'project', 'is', 'amazing', '!', 'It', \"'s\", 'a', 'beautiful', 'wworld', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our', 'o', 'his', 'were', 'had', 'its', 'each', 'doing', 'she', 'haven', 'any', 'them', \"should've\", 'a', 're', 'who', 'what', 'myself', 'again', 'don', 'because', 'own', 'weren', 's', 'yourself', 'hers', 'it', 'yourselves', 'about', 'under', 'how', \"it's\", \"couldn't\", 'her', 'he', 'ain', 'whom', 'but', 'here', 'on', 'against', 'such', 'from', 'too', 'other', 'they', \"needn't\", 'does', 'or', 'aren', 'with', 'd', \"haven't\", 'most', 'off', 'hadn', 'in', 'if', 'at', 'there', 'can', 'him', 'the', 'same', 'ours', 'an', 'why', 'only', 'then', 'when', 'are', \"you'll\", 't', \"aren't\", \"wasn't\", 'through', 'ma', 'was', 'having', 'further', \"you'd\", \"didn't\", 'for', \"hasn't\", \"isn't\", 'all', 'more', 'shouldn', 'than', 'and', 'ourselves', \"shan't\", \"shouldn't\", 'me', 'couldn', 'nor', 'yours', 'until', 'my', 'wouldn', \"she's\", 'we', 'as', \"mustn't\", 'now', \"mightn't\", 'both', 'very', \"hadn't\", 'down', 'doesn', 'y', 'no', 'mustn', \"wouldn't\", 'isn', 'some', 'themselves', 'where', 'is', 'few', 'over', 'their', 'by', 'after', 'up', 'once', \"weren't\", 'theirs', 'these', 'that', 'will', 'not', 'of', 'hasn', 'into', 'during', 'needn', 'so', 'itself', 'shan', 'himself', 'this', 'll', 'out', 'above', 'those', 'am', 'should', \"doesn't\", 'did', 'which', \"don't\", 'before', 'm', 'been', 'has', 'you', 'below', \"won't\", 'have', \"you've\", 'do', 'wasn', 'to', 'being', 'your', 've', 'just', \"that'll\", 'herself', \"you're\", 'i', 'mightn', 'won', 'be', 'between', 'didn', 'while'}\n"
     ]
    }
   ],
   "source": [
    "# In NLP useless words are known as stopwords, useless data\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'usage', 'meaning', 'stopwords', 'data', 'preprocessing']\n",
      "['This', 'is', 'a', 'an', 'example', 'showing', 'the', 'usage', 'and', 'meaning', 'of', 'stopwords', 'for', 'data', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "example=\"This is a an example showing the usage and meaning of stopwords for data preprocessing\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "non_filtered_sentence = [w for w in word_tokens]\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(filtered_sentence)\n",
    "print(non_filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'usage', 'meaning', 'stopwords', 'data', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "# Another way of doing this-\n",
    "filtered1 = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered1.append(w)\n",
    "print(filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n",
      "love\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "# Stemming words with NLTK\n",
    "# It is an attempt to normalize sentences, a method of preprocessing again\n",
    "# Often some words have the same meaning, but are written in diffeerent way, like love, loving , e.t.c, like tenses mean the same\n",
    "# but python might consider them as individual identity, this leads to redundancy as well as more compile and run time processing\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example1 = ['love','loving','loved','lovely']\n",
    "\n",
    "for w in example1:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\n",
      "peopl\n",
      "studi\n",
      "well\n",
      ",\n",
      "I\n",
      "tri\n",
      "studi\n",
      "and\n",
      "fall\n",
      "asleep\n",
      "most\n",
      "of\n",
      "the\n",
      "time\n",
      ",\n",
      "studi\n",
      "dont\n",
      "come\n",
      "so\n",
      "handi\n"
     ]
    }
   ],
   "source": [
    "#Stemming of a sentence\n",
    "sent = \"Some people study well, I try studying and fall asleep most of the times, studies dont come so handy\"\n",
    "\n",
    "words = word_tokenize(sent)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech Tagging > basically classifies words as nouns, words, adjectives.. e.t.c\n",
    "# NLTK can also handle tenses\n",
    "# New tokenizer capable of unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr #nltk.download()>> Universal declaration of  human rights in CORPORA\n",
    "print(udhr.raw('English-Latin1'))\n",
    "# Using this document as an example for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text) # Long output cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now training the punktokenizer on the train_text\n",
    "\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using trained model to tokeniize the sample text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized) # Looked att it and cleared the output as it was really long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# defining a process that will tag each tokenized word with a part of speech it belongs to\n",
    "\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:2]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            print(tagger)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "    \n",
    "# NNP for noun , CD cardinal digit # >>https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the tagsets-\n",
    "nltk.help.upenn_tagset() # Seen and cleared content\n",
    "#nltk.download() # Packages> tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the words together \n",
    "# It is basically grouping the words into meaningful clusters that well fit in a particular set of words\n",
    "# >> https://pythonprogramming.net/chunking-nltk-tutorial/\n",
    "# Basic stuff-\n",
    "#'''+ = match 1 or more\n",
    "#? = match 0 or 1 repetitions.\n",
    "#* = match 0 or MORE repetitions\t  \n",
    "#. = Any character except a new line'''\n",
    "\n",
    "\n",
    "# using the ones already worked with\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n",
    "# Explanation of the sentence r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" >\n",
    "#'''\n",
    "#<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \n",
    "\n",
    "#<VB.?>* = \"0 or more of any tense of verb,\" followed by: \n",
    "\n",
    "#<NNP>+ = \"One or more proper nouns,\" followed by \n",
    "\n",
    "#<NN>? = \"zero or one singular noun.\" \n",
    "\n",
    "#'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            # Print the nltk chunk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP)\n",
      "(Chunk ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk noble/JJ dream/NN ./.)\n",
      "(Chunk Tonight/NN we/PRP)\n",
      "(Chunk hope/NN)\n",
      "(Chunk glad/JJ reunion/NN)\n",
      "(Chunk husband/NN who/WP)\n",
      "(Chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
      "(Chunk grateful/JJ)\n",
      "(Chunk good/JJ life/NN)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk his/PRP$ State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.)\n"
     ]
    }
   ],
   "source": [
    "# Moving on to chinking, similar to chunking, basically removing the words in the chunk that we do not want\n",
    "# Chinking with NLTK\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}                            \n",
    "                                         }<VB.?|IN|DT|TO>+{\"\"\"        # Inside out brackets tell us that we do not want these, verbs, determiinant, e.t.c\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            # Print the nltk chunk tree\n",
    "            for subtree in chunked.subtrees(filter = lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n",
    "# No more verbs and all that, just what we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition with NLTK\n",
    "# It identifies names, locations,people,things, places and all that. Very common form of chunking\n",
    "#  We can recogonise all named entities , or we can recogonise them with seperate types\n",
    "\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagger ,binary=False) # binary = true first and then binary=false, \n",
    "                                                          # true gives generalized recognition, false gives specific types\n",
    "                                                          # like organisation, person e.t.c            \n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done till now\n",
    "\n",
    "1. Tokenizing\n",
    "2. Stemming\n",
    "3. Part-of-speech tagging\n",
    "4. Chunking/Chinking\n",
    "5. Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
