{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python : 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) \n",
      "[GCC 7.2.0]\n",
      "Pandas : 0.23.4\n",
      "Sklearn : 0.20.0\n",
      "NLTK :3.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import sklearn\n",
    "\n",
    "print(\"Python : {}\".format(sys.version))\n",
    "print(\"Pandas : {}\".format(pandas.__version__))\n",
    "print(\"Sklearn : {}\".format(sklearn.__version__))\n",
    "print(\"NLTK :{}\".format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # Opens up a nltk downloader with all the packages listed to be seleccted and donloaded easily\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of vocabulary with NLP\n",
    "\n",
    "### Corpus\n",
    "Body of the text,singular. Corpora is it's plural. ex: A collection of comic books\n",
    "\n",
    "### Lexicon\n",
    "Words and their meanings with respect to different organisations and types of people from various backgrounds\n",
    "\n",
    "### Token\n",
    "Single entitiy that has been split from the document to be used or has been split up based on the rules, example: each word is atoken when a sentence is tokenized. A sentence can be atoken when it's a part of a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "text = \"Hey dude, what's up, how are you doing today! I think this project is amazing! It's a beautiful wworld.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey dude, what's up, how are you doing today!\", 'I think this project is amazing!', \"It's a beautiful wworld.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', 'dude', ',', 'what', \"'s\", 'up', ',', 'how', 'are', 'you', 'doing', 'today', '!', 'I', 'think', 'this', 'project', 'is', 'amazing', '!', 'It', \"'s\", 'a', 'beautiful', 'wworld', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'our', 'o', 'his', 'were', 'had', 'its', 'each', 'doing', 'she', 'haven', 'any', 'them', \"should've\", 'a', 're', 'who', 'what', 'myself', 'again', 'don', 'because', 'own', 'weren', 's', 'yourself', 'hers', 'it', 'yourselves', 'about', 'under', 'how', \"it's\", \"couldn't\", 'her', 'he', 'ain', 'whom', 'but', 'here', 'on', 'against', 'such', 'from', 'too', 'other', 'they', \"needn't\", 'does', 'or', 'aren', 'with', 'd', \"haven't\", 'most', 'off', 'hadn', 'in', 'if', 'at', 'there', 'can', 'him', 'the', 'same', 'ours', 'an', 'why', 'only', 'then', 'when', 'are', \"you'll\", 't', \"aren't\", \"wasn't\", 'through', 'ma', 'was', 'having', 'further', \"you'd\", \"didn't\", 'for', \"hasn't\", \"isn't\", 'all', 'more', 'shouldn', 'than', 'and', 'ourselves', \"shan't\", \"shouldn't\", 'me', 'couldn', 'nor', 'yours', 'until', 'my', 'wouldn', \"she's\", 'we', 'as', \"mustn't\", 'now', \"mightn't\", 'both', 'very', \"hadn't\", 'down', 'doesn', 'y', 'no', 'mustn', \"wouldn't\", 'isn', 'some', 'themselves', 'where', 'is', 'few', 'over', 'their', 'by', 'after', 'up', 'once', \"weren't\", 'theirs', 'these', 'that', 'will', 'not', 'of', 'hasn', 'into', 'during', 'needn', 'so', 'itself', 'shan', 'himself', 'this', 'll', 'out', 'above', 'those', 'am', 'should', \"doesn't\", 'did', 'which', \"don't\", 'before', 'm', 'been', 'has', 'you', 'below', \"won't\", 'have', \"you've\", 'do', 'wasn', 'to', 'being', 'your', 've', 'just', \"that'll\", 'herself', \"you're\", 'i', 'mightn', 'won', 'be', 'between', 'didn', 'while'}\n"
     ]
    }
   ],
   "source": [
    "# In NLP useless words are known as stopwords, useless data\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'usage', 'meaning', 'stopwords', 'data', 'preprocessing']\n",
      "['This', 'is', 'a', 'an', 'example', 'showing', 'the', 'usage', 'and', 'meaning', 'of', 'stopwords', 'for', 'data', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "example=\"This is a an example showing the usage and meaning of stopwords for data preprocessing\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "non_filtered_sentence = [w for w in word_tokens]\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(filtered_sentence)\n",
    "print(non_filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'usage', 'meaning', 'stopwords', 'data', 'preprocessing']\n"
     ]
    }
   ],
   "source": [
    "# Another way of doing this-\n",
    "filtered1 = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered1.append(w)\n",
    "print(filtered1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n",
      "love\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "# Stemming words with NLTK\n",
    "# It is an attempt to normalize sentences, a method of preprocessing again\n",
    "# Often some words have the same meaning, but are written in diffeerent way, like love, loving , e.t.c, like tenses mean the same\n",
    "# but python might consider them as individual identity, this leads to redundancy as well as more compile and run time processing\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example1 = ['love','loving','loved','lovely']\n",
    "\n",
    "for w in example1:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some\n",
      "peopl\n",
      "studi\n",
      "well\n",
      ",\n",
      "I\n",
      "tri\n",
      "studi\n",
      "and\n",
      "fall\n",
      "asleep\n",
      "most\n",
      "of\n",
      "the\n",
      "time\n",
      ",\n",
      "studi\n",
      "dont\n",
      "come\n",
      "so\n",
      "handi\n"
     ]
    }
   ],
   "source": [
    "#Stemming of a sentence\n",
    "sent = \"Some people study well, I try studying and fall asleep most of the times, studies dont come so handy\"\n",
    "\n",
    "words = word_tokenize(sent)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech Tagging > basically classifies words as nouns, words, adjectives.. e.t.c\n",
    "# NLTK can also handle tenses\n",
    "# New tokenizer capable of unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr #nltk.download()>> Universal declaration of  human rights in CORPORA\n",
    "print(udhr.raw('English-Latin1'))\n",
    "# Using this document as an example for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_text) # Long output cleared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now training the punktokenizer on the train_text\n",
    "\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using trained model to tokeniize the sample text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized) # Looked att it and cleared the output as it was really long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# defining a process that will tag each tokenized word with a part of speech it belongs to\n",
    "\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:2]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            print(tagger)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "    \n",
    "# NNP for noun , CD cardinal digit # >>https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the tagsets-\n",
    "nltk.help.upenn_tagset() # Seen and cleared content\n",
    "#nltk.download() # Packages> tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the words together \n",
    "# It is basically grouping the words into meaningful clusters that well fit in a particular set of words\n",
    "# >> https://pythonprogramming.net/chunking-nltk-tutorial/\n",
    "# Basic stuff-\n",
    "#'''+ = match 1 or more\n",
    "#? = match 0 or 1 repetitions.\n",
    "#* = match 0 or MORE repetitions\t  \n",
    "#. = Any character except a new line'''\n",
    "\n",
    "\n",
    "# using the ones already worked with\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n",
    "# Explanation of the sentence r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\" >\n",
    "#'''\n",
    "#<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \n",
    "\n",
    "#<VB.?>* = \"0 or more of any tense of verb,\" followed by: \n",
    "\n",
    "#<NNP>+ = \"One or more proper nouns,\" followed by \n",
    "\n",
    "#<NN>? = \"zero or one singular noun.\" \n",
    "\n",
    "#'''\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "(Chunk ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk THE/NNP UNION/NNP January/NNP)\n",
      "(Chunk THE/NNP PRESIDENT/NNP)\n",
      "(Chunk Thank/NNP)\n",
      "(Chunk Mr./NNP Speaker/NNP)\n",
      "(Chunk Vice/NNP President/NNP Cheney/NNP)\n",
      "(Chunk Congress/NNP)\n",
      "(Chunk Supreme/NNP Court/NNP)\n",
      "(Chunk called/VBD America/NNP)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP)\n",
      "(Chunk Applause/NNP)\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP)\n",
      "(Chunk Tuesday/NNP)\n",
      "(Chunk Jan/NNP)\n"
     ]
    }
   ],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            # Print the nltk chunk tree\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP 'S/POS ADDRESS/NNP)\n",
      "(Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "(Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "(Chunk\n",
      "  THE/NNP\n",
      "  UNION/NNP\n",
      "  January/NNP\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  THE/NNP\n",
      "  PRESIDENT/NNP\n",
      "  :/:\n",
      "  Thank/NNP\n",
      "  you/PRP)\n",
      "(Chunk ./.)\n",
      "(Chunk\n",
      "  Mr./NNP\n",
      "  Speaker/NNP\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  Cheney/NNP\n",
      "  ,/,\n",
      "  members/NNS)\n",
      "(Chunk Congress/NNP ,/, members/NNS)\n",
      "(Chunk\n",
      "  Supreme/NNP\n",
      "  Court/NNP\n",
      "  and/CC\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  and/CC\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:)\n",
      "(Chunk our/PRP$ nation/NN)\n",
      "(Chunk ,/, graceful/JJ ,/, courageous/JJ woman/NN who/WP)\n",
      "(Chunk America/NNP)\n",
      "(Chunk its/PRP$ founding/NN ideals/NNS and/CC)\n",
      "(Chunk noble/JJ dream/NN ./.)\n",
      "(Chunk Tonight/NN we/PRP)\n",
      "(Chunk hope/NN)\n",
      "(Chunk glad/JJ reunion/NN)\n",
      "(Chunk husband/NN who/WP)\n",
      "(Chunk so/RB long/RB ago/RB ,/, and/CC we/PRP)\n",
      "(Chunk grateful/JJ)\n",
      "(Chunk good/JJ life/NN)\n",
      "(Chunk Coretta/NNP Scott/NNP King/NNP ./.)\n",
      "(Chunk (/( Applause/NNP ./. )/))\n",
      "(Chunk President/NNP George/NNP W./NNP Bush/NNP)\n",
      "(Chunk his/PRP$ State/NNP)\n",
      "(Chunk Union/NNP Address/NNP)\n",
      "(Chunk Capitol/NNP ,/, Tuesday/NNP ,/, Jan/NNP ./.)\n"
     ]
    }
   ],
   "source": [
    "# Moving on to chinking, similar to chunking, basically removing the words in the chunk that we do not want\n",
    "# Chinking with NLTK\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')   # Using the documents bith wriiten by George Bush, one to train and other to use\n",
    "sample_text= state_union.raw('2006-GWBush.txt')\n",
    "\n",
    "# Tokenizing the text , training on it\n",
    "custom_sent_tokenizer= PunktSentenceTokenizer(train_text)\n",
    "\n",
    "# Tokenizing the sample text\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Process tagging function\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            # Combining the part of speech tag with regukar expression\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}                            \n",
    "                                         }<VB.?|IN|DT|TO>+{\"\"\"        # Inside out brackets tell us that we do not want these, verbs, determiinant, e.t.c\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagger)\n",
    "            \n",
    "            # Draw the chunks, cool feature of NLTK\n",
    "            # Print the nltk chunk tree\n",
    "            for subtree in chunked.subtrees(filter = lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "            #chunked.draw()  either change the code to show just one line from the para, or leave it like : for i in para[:2]:\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n",
    "# No more verbs and all that, just what we needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition with NLTK\n",
    "# It identifies names, locations,people,things, places and all that. Very common form of chunking\n",
    "#  We can recogonise all named entities , or we can recogonise them with seperate types\n",
    "\n",
    "def pos_tagger(para):   # Takes a paragraph as input\n",
    "    try:\n",
    "        for i in para[:5]:          # The variable (para)is a paragraph, so just considering first two sentences of the paragraph\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagger = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagger ,binary=False) # binary = true first and then binary=false, \n",
    "                                                          # true gives generalized recognition, false gives specific types\n",
    "                                                          # like organisation, person e.t.c            \n",
    "            namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "pos_tagger(tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done till now\n",
    "\n",
    "1. Tokenizing\n",
    "2. Stemming\n",
    "3. Part-of-speech tagging\n",
    "4. Chunking/Chinking\n",
    "5. Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Text Classification by  classifying movie reviews- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importinh necessary libraries for movie review classification\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus  import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2000\n",
      "First Review:(['best', 'remembered', 'for', 'his', 'understated', 'performance', 'as', 'dr', '.', 'hannibal', 'lecter', 'in', 'michael', 'mann', \"'\", 's', 'forensics', 'thriller', ',', 'manhunter', ',', 'scottish', 'character', 'actor', 'brian', 'cox', 'brings', 'something', 'special', 'to', 'every', 'movie', 'he', 'works', 'on', '.', 'usually', 'playing', 'a', 'bit', 'role', 'in', 'some', 'studio', 'schlock', '(', 'he', 'dies', 'halfway', 'through', 'the', 'long', 'kiss', 'goodnight', ')', ',', 'he', \"'\", 's', 'only', 'occasionally', 'given', 'something', 'meaty', 'and', 'substantial', 'to', 'do', '.', 'if', 'you', 'want', 'to', 'see', 'some', 'brilliant', 'acting', ',', 'check', 'out', 'his', 'work', 'as', 'a', 'dogged', 'police', 'inspector', 'opposite', 'frances', 'mcdormand', 'in', 'ken', 'loach', \"'\", 's', 'hidden', 'agenda', '.', 'cox', 'plays', 'the', 'role', 'of', 'big', 'john', 'harrigan', 'in', 'the', 'disturbing', 'new', 'indie', 'flick', 'l', '.', 'i', '.', 'e', '.', ',', 'which', 'lot', '47', 'picked', 'up', 'at', 'sundance', 'when', 'other', 'distributors', 'were', 'scared', 'to', 'budge', '.', 'big', 'john', 'feels', 'the', 'love', 'that', 'dares', 'not', 'speak', 'its', 'name', ',', 'but', 'he', 'expresses', 'it', 'through', 'seeking', 'out', 'adolescents', 'and', 'bringing', 'them', 'back', 'to', 'his', 'pad', '.', 'what', 'bothered', 'some', 'audience', 'members', 'was', 'the', 'presentation', 'of', 'big', 'john', 'in', 'an', 'oddly', 'empathetic', 'light', '.', 'he', \"'\", 's', 'an', 'even', '-', 'tempered', ',', 'funny', ',', 'robust', 'old', 'man', 'who', 'actually', 'listens', 'to', 'the', 'kids', \"'\", 'problems', '(', 'as', 'opposed', 'to', 'their', 'parents', 'and', 'friends', ',', 'both', 'caught', 'up', 'in', 'the', 'high', '-', 'wire', 'act', 'of', 'their', 'own', 'confused', 'lives', '.', ')', 'he', \"'\", 'll', 'have', 'sex', '-', 'for', '-', 'pay', 'with', 'them', 'only', 'after', 'an', 'elaborate', 'courtship', ',', 'charming', 'them', 'with', 'temptations', 'from', 'the', 'grown', '-', 'up', 'world', '.', 'l', '.', 'i', '.', 'e', '.', 'stands', 'for', 'long', 'island', 'expressway', ',', 'which', 'slices', 'through', 'the', 'strip', 'malls', 'and', 'middle', '-', 'class', 'homes', 'of', 'suburbia', '.', 'filmmaker', 'michael', 'cuesta', 'uses', 'it', 'as', 'a', '(', 'pretty', 'transparent', ')', 'metaphor', 'of', 'dangerous', 'escape', 'for', 'his', '15', '-', 'year', 'old', 'protagonist', ',', 'howie', '(', 'paul', 'franklin', 'dano', ')', '.', 'in', 'his', 'opening', 'voice', '-', 'over', ',', 'howie', 'reveals', 'a', 'morbid', 'preoccupation', 'with', 'death', 'on', 'the', 'road', ',', 'citing', 'the', 'l', '.', 'i', '.', 'e', '.', 'highway', 'deaths', 'of', 'filmmaker', 'alan', 'j', '.', 'pakula', ',', 'songwriter', 'harry', 'chapin', ',', 'and', 'his', 'own', 'mother', 'on', 'exit', '52', '.', 'he', \"'\", 's', 'both', 'fascinated', 'and', 'disturbed', 'by', 'the', 'l', '.', 'i', '.', 'e', '.', ',', 'and', 'those', 'feelings', 'are', 'projected', 'onto', 'big', 'john', '(', 'who', 'follows', 'howie', 'around', 'in', 'his', 'bright', 'red', 'car', ',', 'but', 'never', 'makes', 'a', 'move', 'to', 'force', 'the', 'boy', 'to', 'do', 'something', 'he', 'doesn', \"'\", 't', 'want', 'to', 'do', '.', 'this', 'makes', 'him', 'much', 'more', 'complex', 'than', 'the', 'usual', 'child', 'molesters', 'seen', 'in', 'movies', '--', 'he', \"'\", 's', 'a', 'beast', ',', 'but', 'ashamed', 'of', 'it', '.', ')', 'l', '.', 'i', '.', 'e', '.', 'would', 'have', 'worked', 'best', 'as', 'a', 'half', '-', 'hour', 'short', 'film', 'about', 'howie', \"'\", 's', 'ill', '-', 'advised', 'foray', 'into', 'big', 'john', \"'\", 's', 'haven', '.', 'there', 'is', 'unnecessary', 'padding', 'with', 'howie', \"'\", 's', 'miserable', 'dad', '(', 'bruce', 'altman', ')', 'in', 'the', 'hot', 'seat', 'for', 'a', 'white', '-', 'collar', 'crime', ',', 'degenerate', 'youngsters', 'who', 'get', 'their', 'kicks', 'from', 'robbing', 'middle', '-', 'class', 'houses', ',', 'and', 'some', 'homoerotic', 'shenanigans', 'with', 'wise', '-', 'ass', 'gary', 'terrio', '(', 'billy', 'kay', ')', ',', 'a', 'handsome', 'artful', 'dodger', '.', 'rather', 'than', 'add', 'to', 'the', 'themes', 'of', 'suburban', 'ennui', '(', 'not', 'that', 'we', 'needed', 'another', 'movie', 'on', 'that', 'subject', ')', ',', 'these', 'awkward', 'subplots', 'pad', 'out', 'the', 'running', 'time', 'to', 'adequate', 'feature', 'length', '.', 'concurrently', ',', 'the', 'relationship', 'between', 'howie', 'and', 'big', 'john', 'is', 'evenly', 'paced', 'and', 'exceptionally', 'well', 'acted', '.', 'cox', ',', 'sporting', 'a', 'baseball', 'cap', 'and', 'a', 'faded', 'marine', 'tattoo', ',', 'is', 'all', 'bluff', 'and', 'bluster', '.', 'dano', 'is', 'quiet', 'and', 'at', 'first', 'glance', 'seems', 'so', 'withdrawn', 'as', 'to', 'be', 'transparent', '.', 'we', \"'\", 're', 'so', 'used', 'to', 'child', 'actors', 'whose', 'dramatic', 'choices', 'are', 'broad', 'and', 'obvious', '(', 'calling', 'haley', 'joel', '!', ')', ',', 'it', \"'\", 's', 'surprising', 'to', 'see', 'one', 'who', 'actually', 'listens', 'throughout', 'any', 'given', 'scene', '.', 'the', 'restraint', 'is', 'admirable', '.', 'but', 'l', '.', 'i', '.', 'e', '.', \"'\", 's', 'screenplay', 'doesn', \"'\", 't', 'always', 'give', 'them', 'the', 'best', 'material', '.', 'when', 'howie', 'reads', 'big', 'john', 'a', 'walt', 'whitman', 'poem', ',', 'the', 'moment', 'feels', 'a', 'bit', 'too', 'precious', '.', 'director', 'michael', 'cuesta', 'lingers', 'on', 'an', 'ecstatic', 'reaction', 'shot', 'of', 'big', 'john', ',', 'who', 'may', 'as', 'well', 'be', 'hearing', 'glenn', 'gould', 'performing', 'bach', \"'\", 's', 'goldberg', 'variations', '.', 'it', \"'\", 's', 'too', 'much', '.', 'there', 'are', 'also', 'some', 'obvious', 'dramatic', 'contrivances', 'involving', 'big', 'john', \"'\", 's', 'other', 'boy', 'toy', '(', 'walter', 'masterson', ')', ',', 'jealous', 'over', 'the', 'newbie', '.', 'this', 'plot', 'thread', 'predictably', 'leads', 'to', 'violence', '.', 'not', 'content', 'to', 'be', 'a', 'haunting', ',', 'observational', 'portrait', 'of', 'teen', 'alienation', 'in', 'a', 'royally', 'screwed', 'up', 'world', '(', 'like', 'terry', 'zwigoff', \"'\", 's', 'superb', 'ghost', 'world', ')', ',', 'cuesta', 'lacks', 'the', 'confidence', 'in', 'his', 'own', 'work', 'to', 'end', 'on', 'an', 'ambivalent', 'note', '.', 'it', \"'\", 's', 'typical', 'of', 'unimaginative', 'cinema', 'to', 'wrap', 'things', 'up', 'with', 'a', 'bullet', ',', 'sparing', 'the', 'writers', 'from', 'actually', 'having', 'to', 'come', 'up', 'with', 'a', 'complex', ',', 'philosophical', 'note', '.', 'in', 'this', 'regard', ',', 'l', '.', 'i', '.', 'e', '.', '(', 'and', 'countless', 'other', 'indie', 'films', ')', 'share', 'something', 'in', 'common', 'with', 'blockbuster', 'action', 'films', ':', 'problems', 'are', 'solved', 'when', 'the', 'obstacle', 'is', 'removed', '.', 'how', 'often', 'does', 'real', 'life', 'work', 'this', 'way', '?', 'to', 'extend', 'the', 'question', ':', 'if', 'a', 'movie', 'is', 'striving', 'for', 'realism', ',', 'do', 'dramatic', 'contrivances', 'destroy', 'the', 'illusion', '?'], 'neg')\n",
      "Most common words: [(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n",
      "the word happy :215\n"
     ]
    }
   ],
   "source": [
    "# Building a list of documents \n",
    "docs = [(list(movie_reviews.words(fileid)),category)\n",
    "        for category in movie_reviews.categories()\n",
    "        for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents to change ordeer and not skew the results\n",
    "random.shuffle(docs)\n",
    "# Palying with docs\n",
    "print(\"Number of documents: {}\".format(len(docs)))\n",
    "print(\"First Review:{}\".format(docs[0]))\n",
    "\n",
    "# Building a list of all the words contained\n",
    "\n",
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(\"Most common words: {}\".format(all_words.most_common(15)))\n",
    "print(\"the word happy :{}\".format(all_words[\"happy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39768\n"
     ]
    }
   ],
   "source": [
    "# Prinitng the length of all words\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the most common words as features\n",
    "word_features = list(all_words.keys())[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot\n",
      ":\n",
      "two\n",
      "teen\n",
      "couples\n",
      "go\n",
      "to\n",
      "a\n",
      "church\n",
      "party\n",
      ",\n",
      "drink\n",
      "and\n",
      "then\n",
      "drive\n",
      ".\n",
      "they\n",
      "get\n",
      "into\n",
      "an\n",
      "accident\n",
      "one\n",
      "of\n",
      "the\n",
      "guys\n",
      "dies\n",
      "but\n",
      "his\n",
      "girlfriend\n",
      "continues\n",
      "see\n",
      "him\n",
      "in\n",
      "her\n",
      "life\n",
      "has\n",
      "nightmares\n",
      "what\n",
      "'\n",
      "s\n",
      "deal\n",
      "?\n",
      "watch\n",
      "movie\n",
      "\"\n",
      "sorta\n",
      "find\n",
      "out\n",
      "critique\n",
      "mind\n",
      "-\n",
      "fuck\n",
      "for\n",
      "generation\n",
      "that\n",
      "touches\n",
      "on\n",
      "very\n",
      "cool\n",
      "idea\n",
      "presents\n",
      "it\n",
      "bad\n",
      "package\n",
      "which\n",
      "is\n",
      "makes\n",
      "this\n",
      "review\n",
      "even\n",
      "harder\n",
      "write\n",
      "since\n",
      "i\n",
      "generally\n",
      "applaud\n",
      "films\n",
      "attempt\n",
      "break\n",
      "mold\n",
      "mess\n",
      "with\n",
      "your\n",
      "head\n",
      "such\n",
      "(\n",
      "lost\n",
      "highway\n",
      "&\n",
      "memento\n",
      ")\n",
      "there\n",
      "are\n",
      "good\n",
      "ways\n",
      "making\n",
      "all\n",
      "types\n",
      "these\n",
      "folks\n",
      "just\n",
      "didn\n",
      "t\n",
      "snag\n",
      "correctly\n",
      "seem\n",
      "have\n",
      "taken\n",
      "pretty\n",
      "neat\n",
      "concept\n",
      "executed\n",
      "terribly\n",
      "so\n",
      "problems\n",
      "well\n",
      "its\n",
      "main\n",
      "problem\n",
      "simply\n",
      "too\n",
      "jumbled\n",
      "starts\n",
      "off\n",
      "normal\n",
      "downshifts\n",
      "fantasy\n",
      "world\n",
      "you\n",
      "as\n",
      "audience\n",
      "member\n",
      "no\n",
      "going\n",
      "dreams\n",
      "characters\n",
      "coming\n",
      "back\n",
      "from\n",
      "dead\n",
      "others\n",
      "who\n",
      "look\n",
      "like\n",
      "strange\n",
      "apparitions\n",
      "disappearances\n",
      "looooot\n",
      "chase\n",
      "scenes\n",
      "tons\n",
      "weird\n",
      "things\n",
      "happen\n",
      "most\n",
      "not\n",
      "explained\n",
      "now\n",
      "personally\n",
      "don\n",
      "trying\n",
      "unravel\n",
      "film\n",
      "every\n",
      "when\n",
      "does\n",
      "give\n",
      "me\n",
      "same\n",
      "clue\n",
      "over\n",
      "again\n",
      "kind\n",
      "fed\n",
      "up\n",
      "after\n",
      "while\n",
      "biggest\n",
      "obviously\n",
      "got\n",
      "big\n",
      "secret\n",
      "hide\n",
      "seems\n",
      "want\n",
      "completely\n",
      "until\n",
      "final\n",
      "five\n",
      "minutes\n",
      "do\n",
      "make\n",
      "entertaining\n",
      "thrilling\n",
      "or\n",
      "engaging\n",
      "meantime\n",
      "really\n",
      "sad\n",
      "part\n",
      "arrow\n",
      "both\n",
      "dig\n",
      "flicks\n",
      "we\n",
      "actually\n",
      "figured\n",
      "by\n",
      "half\n",
      "way\n",
      "point\n",
      "strangeness\n",
      "did\n",
      "start\n",
      "little\n",
      "bit\n",
      "sense\n",
      "still\n",
      "more\n",
      "guess\n",
      "bottom\n",
      "line\n",
      "movies\n",
      "should\n",
      "always\n",
      "sure\n",
      "before\n",
      "given\n",
      "password\n",
      "enter\n",
      "understanding\n",
      "mean\n",
      "showing\n",
      "melissa\n",
      "sagemiller\n",
      "running\n",
      "away\n",
      "visions\n",
      "about\n",
      "20\n",
      "throughout\n",
      "plain\n",
      "lazy\n",
      "!\n",
      "okay\n",
      "people\n",
      "chasing\n",
      "know\n",
      "need\n",
      "how\n",
      "giving\n",
      "us\n",
      "different\n",
      "offering\n",
      "further\n",
      "insight\n",
      "down\n",
      "apparently\n",
      "studio\n",
      "took\n",
      "director\n",
      "chopped\n",
      "themselves\n",
      "shows\n",
      "might\n",
      "ve\n",
      "been\n",
      "decent\n",
      "here\n",
      "somewhere\n",
      "suits\n",
      "decided\n",
      "turning\n",
      "music\n",
      "video\n",
      "edge\n",
      "would\n",
      "actors\n",
      "although\n",
      "wes\n",
      "bentley\n",
      "seemed\n",
      "be\n",
      "playing\n",
      "exact\n",
      "character\n",
      "he\n",
      "american\n",
      "beauty\n",
      "only\n",
      "new\n",
      "neighborhood\n",
      "my\n",
      "kudos\n",
      "holds\n",
      "own\n",
      "entire\n",
      "feeling\n",
      "unraveling\n",
      "overall\n",
      "doesn\n",
      "stick\n",
      "because\n",
      "entertain\n",
      "confusing\n",
      "rarely\n",
      "excites\n",
      "feels\n",
      "redundant\n",
      "runtime\n",
      "despite\n",
      "ending\n",
      "explanation\n",
      "craziness\n",
      "came\n",
      "oh\n",
      "horror\n",
      "slasher\n",
      "flick\n",
      "packaged\n",
      "someone\n",
      "assuming\n",
      "genre\n",
      "hot\n",
      "kids\n",
      "also\n",
      "wrapped\n",
      "production\n",
      "years\n",
      "ago\n",
      "sitting\n",
      "shelves\n",
      "ever\n",
      "whatever\n",
      "skip\n",
      "where\n",
      "joblo\n",
      "nightmare\n",
      "elm\n",
      "street\n",
      "3\n",
      "7\n",
      "/\n",
      "10\n",
      "blair\n",
      "witch\n",
      "2\n",
      "crow\n",
      "9\n",
      "salvation\n",
      "4\n",
      "stir\n",
      "echoes\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Building a find_features function that will determine which of the 4000 word features are contained in a review\n",
    "def find_features(docs):\n",
    "    words=set(docs)\n",
    "    features = {}\n",
    "    \n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "# Using an example from neagtive review\n",
    "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
    "for key,value in features.items():\n",
    "    if value == True:\n",
    "        print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same for all the documents in our data\n",
    "feature_sets = [(find_features(rev),category) for (rev,category) in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the featuresets into training and test datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Seed defining\n",
    "seed = 1\n",
    "# Splitting the data\n",
    "training,testing = model_selection.train_test_split(feature_sets,test_size = 0.25 , random_state=seed)\n",
    "# We dont have an X and a Y we just have feature_sets, thus we divide it only into  two, training and testing sets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn algorithms in nltk\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model on training data\n",
    "model.train(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC accuracy 0.826\n"
     ]
    }
   ],
   "source": [
    "# Tesing on the test dataset\n",
    "accuracy = nltk.classify.accuracy(model,testing)\n",
    "print('SVC accuracy {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
